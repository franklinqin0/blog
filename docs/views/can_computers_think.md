---
title: Can Computer Think?
date: 2020-09-30
categories:
  - study
tags:
  - tech
  - paper
publish: false
---

Artificial Intelligence (AI), computer programs that perform tasks with human intelligence, is prevalent in this world. From 1957, when Frank Rosenblatt came up with the first machine learning algorithm, perceptron, a binary classifier which later proved to be unable to learn XOR function, to 2016, when DeepMind’s AlphaGo beat 18-time world champion Lee Sedol, AI undeniably has gone a long way and served essential roles in our everyday life. There are programs that master Chess, Backgammon, Bridge, and Go; predict chemical reactions; recognize images; translate languages; read handwriting; write poems, compose music; draw paintings. There are also nearing but concerning applications such as self-driving cars, mass surveillance, and companion robots. Computers will only be more prevalent in the foreseeable future. As AI becomes smarter, we naturally come up with the leading question: can computers think (like a human)?

Alan Turing, the father of computer science and AI, proposed a test for machine's ability to think: imitation game, or _Turing test_[1]. The test would rule computer as a human if $C$, a human interrogator, cannot see the difference in response between player $A$, a computer, and player $B$, a human. The decision, based on observation of responses, however, has multiple caveats.

Firstly, the calibre of the interrogator is critical: naturally, the questions from a 5-year child vs. those from a philosopher would differ. Secondly, it is possible for $C$ to rule computer as a human (false positive) and human as a computer (false negative). Thirdly, the number of possible questions asked by the interrogator is practically limited, so if the computer can remember a manual of limited questions and corresponding answers, it might fool the interrogator.

Of course, the first and second problems can be resolved if there are sufficiently large number of interrogators and questions, so that a hypothesis test can be conducted to eliminate inaccuracies due to individual differences. The third question would lead to the _Chinese Room_ thought experiment[2].

The Chinese Room is a argument that if a computer can take an input of Chinese characters from a Chinese speaker outside the room, follow the instructions of a computer program, and produce an output of some Chinese characters as response, it can fool the Chinese speaker, pass the Turing test, and said to be able to "think". But does the computer "think" to produce the seemingly correct answers? Does it literally "understand" Chinese? Or is it merely simulating the ability to understand Chinese? The evident answers to the above questions can safely refute the effectiveness of Turing test, which concludes that AI could "think" if producing valid answers.

Before we proceed with more counter-arguements, let us consider the fundamental definition of "think". According to René Descartes, a "thinking thing" is "a thing that doubts, understands, affirms, denies, is willing, is unwilling, and also imagines and has sense perceptions". By his definition, a computer definitely can not "think", at least in the same way that humans do.

When a computer "listens" to music, "sees" an image, or "drives" a car, it receives input signals in time domain, converts them to frequency domain, uses some clever algorithms, such as deep learning, to classify or predict results based on training data. And if the training data is lacking or computing power is insufficient, the model cannot reach an optimal state and produce effective results.

These claims can be backed by real-world examples. In 2015, Google, a forerunner in commercial AI, was blamed for classifying some African Americans in a photo as gorillas[3]. Nearly 3 years later, the company still hasn’t really fixed anything, but simply blocked its image recognition algorithms from identifying gorillas altogether. Moreover, primates such as "gorilla", "chimp", "chimpanzee", and "monkey" remained blocked on Google Photos. Though there have been many impressive attempts on interpretablity[6]&[7], no simple and quick adjustment exists for deep learning algorithms with a black-box nature. Another example is Tesla's autopilot system, having caused multiple accidents already[4]. Yet Elon Musk boasted that Tesla engineers would "complete the basics of a completely autonomous driving system this year". The best algorithm we can analog to human intelligence, deep learning, would fail occasionally and not necessarily be as flexible as a human to make logical decision in an unprecedented situation.

Can computers, given enough time, one day become a "thinking thing" by Descartes' definition? To answer this question, we have to acknowledge the fundamental differences between humans and computers. Humans live in a physical world; perceive electromagnetic waves with narrow wavelengths bewteen 400nm and 780nm; have gender, family, lovers, and friends; have emotions like love, hatred, ecstasy, and sorrow; most importantly, humans can perceive so many different forms of data, namely, sight, sound, smell, taste, touch. In comparison, computers can only perceive data encoding in numbers. A phone chatbot mimics a female voice but is not female; it loves to eat ice cream but has no taste. There is no fundamental difference between the way an abacus, a mechanical computer, a electrical computer, or even a quantum computer in terms of data encoding: numbers. Even the state-of-the-art machine learning algorithms, such as BERT[?] in natural language processing, YOLO[?] in object detection, and ShuffleNet[?] in mobile applications, "think" in numbers and only excel at specific tasks (weak AI). Therefore, regretably or auspiciously, human intelligence (strong AI) may never befall as long as a numeric system is essential in the way computers store and process data.

A further question can be raised naturally: if a computer cannot perceive data as we do, is it still possible to have "human intelligence"? Brain-computer Interface (BCI) can bring us hope. BCI is a bidirectional communication pathway between a wired brain and an external computer. There are non-invasive BCI's such as electroencephalography (EEG) that acquires brain wave signals and translates them into useful commands, and transcranial magnetic stimulation (TMS) that translates digital information into electric signals and feeds them to brains via stimulators. Another appraoch is to build an invasive BCI, such as electrocorticography (ECoG) or intracortical microelectrodes. Neuralink, a company founded by Elon Musk, introduced the new technology of minimally invasive brain implant that offers communication access and mobility to patients with spinal cord injuries. With more efforts from neuroscientists, surgeons, electrical and computer engineers, there is a bright hope that a symbiosis of humans and computers will bring unprecedented ease of acquiring knowledge without typing and conducting communications without talking.

With a synergy of humans' various senses and computers' strict logic, BCI can enhance "thinking" of humans and computers. Therefore, there would no need for Turing test, because humans are computers, and computers are humans.
