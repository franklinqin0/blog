(window.webpackJsonp=window.webpackJsonp||[]).push([[229],{749:function(t,e,a){"use strict";a.r(e);var s=a(3),r=Object(s.a)({},(function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h2",{attrs:{id:"classification"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#classification"}},[t._v("#")]),t._v(" Classification")]),t._v(" "),a("p",[t._v("Classification is the task of choosing the correct class label for a given input (= instance) based on its features, e.g.:")]),t._v(" "),a("ul",[a("li",[t._v("Email spam classification")]),t._v(" "),a("li",[t._v("Categorizing news articles by topic")])]),t._v(" "),a("h3",{attrs:{id:"classification-variants"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#classification-variants"}},[t._v("#")]),t._v(" Classification variants")]),t._v(" "),a("ul",[a("li",[t._v("binary (2 classes) vs. multi-class classification (> 2 classes)\n"),a("ul",[a("li",[t._v("multi-class problem can be decomposed using binary classifiers")])])]),t._v(" "),a("li",[t._v("single-label vs. multi-label classification\n"),a("ul",[a("li",[t._v("each instance may be assigned one vs. multiple class labels")])])]),t._v(" "),a("li",[t._v("sequence classification\n"),a("ul",[a("li",[t._v("a sequence of instances are jointly classified")])])])]),t._v(" "),a("h2",{attrs:{id:"read"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#read"}},[t._v("#")]),t._v(" READ")]),t._v(" "),a("p",[a("a",{attrs:{href:""}},[t._v("https://lionbridge.ai/articles/6-types-of-neural-networks-every-data-scientist-must-know/")])]),t._v(" "),a("p",[a("a",{attrs:{href:"https://developer.aliyun.com/learning/course/15/detail/39",target:"_blank",rel:"noopener noreferrer"}},[t._v("Alibaba Redis"),a("OutboundLink")],1)]),t._v(" "),a("p",[a("a",{attrs:{href:"https://tianchi.aliyun.com/specials/promotion/aicamprl?spm=5176.14154004.J_1266466330.4.31fe5699pG0gvu",target:"_blank",rel:"noopener noreferrer"}},[t._v("Alibaba RL"),a("OutboundLink")],1)]),t._v(" "),a("p",[t._v("https://cs231n.github.io/convolutional-networks/")]),t._v(" "),a("p",[a("a",{attrs:{href:"https://ruder.io/optimizing-gradient-descent",target:"_blank",rel:"noopener noreferrer"}},[t._v("detailed overview of optimizers and their development"),a("OutboundLink")],1)]),t._v(" "),a("p",[a("a",{attrs:{href:"https://youtu.be/dtvM-CzNe50",target:"_blank",rel:"noopener noreferrer"}},[t._v("Tensors for Beginners 7: Linear Maps"),a("OutboundLink")],1)]),t._v(" "),a("p",[a("a",{attrs:{href:"https://www.deeplearning.ai/ai-notes/initialization",target:"_blank",rel:"noopener noreferrer"}},[t._v("Xavier initialization"),a("OutboundLink")],1)]),t._v(" "),a("p",[a("a",{attrs:{href:"https://optuna.readthedocs.io/en/stable/tutorial/index.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("optuna"),a("OutboundLink")],1)]),t._v(" "),a("p",[a("a",{attrs:{href:"https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("Tensorboard tutorial"),a("OutboundLink")],1)]),t._v(" "),a("p",[a("a",{attrs:{href:"https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("Autograd"),a("OutboundLink")],1)]),t._v(" "),a("p",[a("a",{attrs:{href:"https://www.analyticsvidhya.com/blog/2018/03/introduction-regression-splines-python-codes",target:"_blank",rel:"noopener noreferrer"}},[t._v("spline"),a("OutboundLink")],1)]),t._v(" "),a("p",[a("a",{attrs:{href:"https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0",target:"_blank",rel:"noopener noreferrer"}},[t._v("different losses"),a("OutboundLink")],1)]),t._v(" "),a("p",[a("a",{attrs:{href:"https://github.com/bentrevett/pytorch-sentiment-analysis",target:"_blank",rel:"noopener noreferrer"}},[t._v("Sentiment Analysis series"),a("OutboundLink")],1)]),t._v(" "),a("h2",{attrs:{id:"watch"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#watch"}},[t._v("#")]),t._v(" Watch")]),t._v(" "),a("p",[a("a",{attrs:{href:"https://youtu.be/xxHkbWMILjI",target:"_blank",rel:"noopener noreferrer"}},[t._v("Andrew Ng Case Studies"),a("OutboundLink")],1),t._v(" "),a("a",{attrs:{href:"https://youtu.be/lvoHnicueoE",target:"_blank",rel:"noopener noreferrer"}},[t._v("Deep RL"),a("OutboundLink")],1)]),t._v(" "),a("p",[a("a",{attrs:{href:"https://youtu.be/0MtwqhIwdrI?t=548",target:"_blank",rel:"noopener noreferrer"}},[t._v("GS - Orthogonal Matrix"),a("OutboundLink")],1)]),t._v(" "),a("p",[a("a",{attrs:{href:"https://www.youtube.com/watch?v=NmM4pv8uQwI",target:"_blank",rel:"noopener noreferrer"}},[t._v("Jump Point Search"),a("OutboundLink")],1)]),t._v(" "),a("h2",{attrs:{id:"pytorch"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#pytorch"}},[t._v("#")]),t._v(" PyTorch")]),t._v(" "),a("h3",{attrs:{id:"foundations"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#foundations"}},[t._v("#")]),t._v(" Foundations")]),t._v(" "),a("p",[a("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/53927068",target:"_blank",rel:"noopener noreferrer"}},[t._v("Pytorch 网络构造"),a("OutboundLink")],1)]),t._v(" "),a("h3",{attrs:{id:"witj-torch-no-grad-vs-requires-grad"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#witj-torch-no-grad-vs-requires-grad"}},[t._v("#")]),t._v(" "),a("code",[t._v("witj torch.no_grad()")]),t._v(" vs. "),a("code",[t._v("requires_grad")])]),t._v(" "),a("p",[a("code",[t._v("with torch.no_grad()")]),t._v(" is a context manager and is used to prevent calculating gradients in the following code block."),a("br"),t._v("\nUsually it is used when you evaluate your model and don’t need to call "),a("code",[t._v("backward()")]),t._v(" to calculate the gradients and update the corresponding parameters. Also, you can use it to initialize the weights with "),a("code",[t._v("torch.nn.init")]),t._v(" functions, since you don’t need the gradients there either.")]),t._v(" "),a("p",[a("code",[t._v("requires_grad")]),t._v(" on the other hand is used when creating a tensor, which should require gradients. Usually you don’t need this in the beginning, as all parameters which require gradients are already wrapped in "),a("code",[t._v("nn.Modules")]),t._v(" in the "),a("code",[t._v("nn")]),t._v(" package. You could set this property e.g. on your input tensor, if you need to update your input for example in an "),a("em",[t._v("adversarial training")]),t._v(" setup.")]),t._v(" "),a("h3",{attrs:{id:"eval"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#eval"}},[t._v("#")]),t._v(" "),a("code",[t._v("eval")])]),t._v(" "),a("p",[a("code",[t._v("eval()")]),t._v(" from "),a("code",[t._v("nn.Module")]),t._v("\nSets the module in evaluation mode.")]),t._v(" "),a("p",[t._v("This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.")]),t._v(" "),a("p",[t._v("This is equivalent with "),a("code",[t._v("self.train(False)")]),t._v(".")]),t._v(" "),a("h3",{attrs:{id:"back-propagation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#back-propagation"}},[t._v("#")]),t._v(" Back Propagation")]),t._v(" "),a("ul",[a("li",[a("code",[t._v("optim.zero_grad()")]),t._v(" clears old gradients from the last step (otherwise you’d just accumulate the gradients from all loss.backward() calls).")]),t._v(" "),a("li",[a("code",[t._v("loss.backward()")]),t._v(" computes the derivative of the loss w.r.t. the parameters (or anything requiring gradients) using backpropagation.")]),t._v(" "),a("li",[a("code",[t._v("opt.step()")]),t._v(" causes the optimizer to take a step based on the gradients of the parameters.")])]),t._v(" "),a("h3",{attrs:{id:"transposed-convolution"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#transposed-convolution"}},[t._v("#")]),t._v(" Transposed Convolution")]),t._v(" "),a("ul",[a("li",[a("a",{attrs:{href:"https://towardsdatascience.com/transposed-convolution-demystified-84ca81b4baba",target:"_blank",rel:"noopener noreferrer"}},[t._v("Transposed Convolution Demystified"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://towardsdatascience.com/what-is-transposed-convolutional-layer-40e5e6e31c11",target:"_blank",rel:"noopener noreferrer"}},[t._v("What is Transposed Convolution"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://distill.pub/2016/deconv-checkerboard/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Checkerboard Artifacts"),a("OutboundLink")],1)])]),t._v(" "),a("h3",{attrs:{id:"autoencoder"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#autoencoder"}},[t._v("#")]),t._v(" Autoencoder")]),t._v(" "),a("p",[t._v("For each image input, the autoencoder just tries to reproduce the same image as output. The difficulty behind is that the autoencoder has to go through a low dimensional bottleneck, which we call the latent space. In other words, the autoencoder should learn to represent all the input information in the low dimensional latent space; it learns to compress the input distribution. To make our model learn to reproduce the input, we use the mean squared error between our input pixels and the output pixels as the loss function. For this loss we do not need any labels!")]),t._v(" "),a("p",[t._v("AutoEncoder 形式很简单, 分别是 encoder 和 decoder, 压缩和解压, 压缩后得到压缩的特征值, 再从压缩的特征值解压成原图片")]),t._v(" "),a("p",[t._v("用 decoder 的信息输出看和原图片的对比,\n还能用 encoder 来看经过压缩后, 神经网络对原图片的理解. encoder 能将不同图片数据大概的分离开来.\n这样就是一个无监督学习的过程.")]),t._v(" "),a("h3",{attrs:{id:"vae"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#vae"}},[t._v("#")]),t._v(" VAE")]),t._v(" "),a("p",[a("a",{attrs:{href:"https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73",target:"_blank",rel:"noopener noreferrer"}},[t._v("Understanding Variational Autoencoders"),a("OutboundLink")],1)]),t._v(" "),a("h3",{attrs:{id:"batch-normalization"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#batch-normalization"}},[t._v("#")]),t._v(" Batch Normalization")]),t._v(" "),a("p",[t._v("Machine learning methods tend to work better when their input data consists of uncorrelated features with zero mean and unit variance. When training a neural network, we can preprocess the data before feeding it to the network to explicitly decorrelate its features; this will ensure that the first layer of the network sees data that follows a nice distribution. However even if we preprocess the input data, the activations at deeper layers of the network will likely no longer be decorrelated and will no longer have zero mean or unit variance since they are output from earlier layers in the network. Even worse, during the training process the distribution of features at each layer of the network will shift as the weights of each layer are updated.")]),t._v(" "),a("p",[t._v("The authors of "),a("sup",{staticClass:"footnote-ref"},[a("a",{attrs:{href:"#fn1",id:"fnref1"}},[t._v("[1]")])]),t._v(" hypothesize that the shifting distribution of features inside deep neural networks may make training deep networks more difficult. To overcome this problem, "),a("sup",{staticClass:"footnote-ref"},[a("a",{attrs:{href:"#fn1",id:"fnref1:1"}},[t._v("[1:1]")])]),t._v(" proposes to insert batch normalization layers into the network. At training time, a batch normalization layer uses a minibatch of data to estimate the mean and standard deviation of each feature. These estimated means and standard deviations are then used to center and normalize the features of the minibatch. A running average of these means and standard deviations is kept during training, and at test time these running averages are used to center and normalize features.")]),t._v(" "),a("p",[t._v("It is possible that this normalization strategy could reduce the representational power of the network, since it may sometimes be optimal for certain layers to have features that are not zero-mean or unit variance. To this end, the batch normalization layer includes learnable shift and scale parameters for each feature dimension.")]),t._v(" "),a("h3",{attrs:{id:"dropout"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#dropout"}},[t._v("#")]),t._v(" Dropout")]),t._v(" "),a("p",[t._v("Dropout [^2] is a technique for regularizing neural networks by randomly setting some features to zero during the forward pass. While training, dropout is implemented by only keeping a neuron active with some probability p (a hyperparameter), or setting it to zero otherwise. The Dropout technique would help your Neural Network to perform better on test data.")]),t._v(" "),a("h3",{attrs:{id:"vanishing-gradient-problem"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#vanishing-gradient-problem"}},[t._v("#")]),t._v(" Vanishing Gradient Problem")]),t._v(" "),a("ul",[a("li",[a("p",[t._v("If weights are initialized with very high values, the term "),a("eq",[a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("mi",[t._v("X")]),a("mi",[t._v("w")]),a("mo",[t._v("+")]),a("mi",[t._v("b")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("Xw+b")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.76666em","vertical-align":"-0.08333em"}}),a("span",{staticClass:"mord mathdefault",staticStyle:{"margin-right":"0.07847em"}},[t._v("X")]),a("span",{staticClass:"mord mathdefault",staticStyle:{"margin-right":"0.02691em"}},[t._v("w")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}}),a("span",{staticClass:"mbin"},[t._v("+")]),a("span",{staticClass:"mspace",staticStyle:{"margin-right":"0.2222222222222222em"}})]),a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.69444em","vertical-align":"0em"}}),a("span",{staticClass:"mord mathdefault"},[t._v("b")])])])])]),t._v(" becomes significantly higher and with activation function such as "),a("eq",[a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("mi",[t._v("t")]),a("mi",[t._v("a")]),a("mi",[t._v("n")]),a("mi",[t._v("h")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("tanh")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.69444em","vertical-align":"0em"}}),a("span",{staticClass:"mord mathdefault"},[t._v("t")]),a("span",{staticClass:"mord mathdefault"},[t._v("a")]),a("span",{staticClass:"mord mathdefault"},[t._v("n")]),a("span",{staticClass:"mord mathdefault"},[t._v("h")])])])])]),t._v(", the function returns value very close to "),a("eq",[a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("mo",[t._v("−")]),a("mn",[t._v("1")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("-1")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.72777em","vertical-align":"-0.08333em"}}),a("span",{staticClass:"mord"},[t._v("−")]),a("span",{staticClass:"mord"},[t._v("1")])])])])]),t._v(" or "),a("eq",[a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("mn",[t._v("1")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("1")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.64444em","vertical-align":"0em"}}),a("span",{staticClass:"mord"},[t._v("1")])])])])]),t._v(". At these values, the gradient of "),a("eq",[a("span",{staticClass:"katex"},[a("span",{staticClass:"katex-mathml"},[a("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[a("semantics",[a("mrow",[a("mi",[t._v("t")]),a("mi",[t._v("a")]),a("mi",[t._v("n")]),a("mi",[t._v("h")])],1),a("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("tanh")])],1)],1)],1),a("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[a("span",{staticClass:"base"},[a("span",{staticClass:"strut",staticStyle:{height:"0.69444em","vertical-align":"0em"}}),a("span",{staticClass:"mord mathdefault"},[t._v("t")]),a("span",{staticClass:"mord mathdefault"},[t._v("a")]),a("span",{staticClass:"mord mathdefault"},[t._v("n")]),a("span",{staticClass:"mord mathdefault"},[t._v("h")])])])])]),t._v(" is very low, thus learning takes a lot of time.")],1)]),t._v(" "),a("li",[a("p",[t._v("If weights are initialized with low values, it gets mapped to around 0, and the small values will kill gradients when backpropagating through the network.")])])]),t._v(" "),a("h2",{attrs:{id:"explanable-ai"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#explanable-ai"}},[t._v("#")]),t._v(" Explanable AI")]),t._v(" "),a("p",[t._v("https://christophm.github.io/interpretable-ml-book/\nhttps://github.com/lopusz/awesome-interpretable-machine-learning\nhttps://stats.stackexchange.com/questions/349319/resources-on-explainable-ai")]),t._v(" "),a("h2",{attrs:{id:"references"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#references"}},[t._v("#")]),t._v(" References")]),t._v(" "),a("p",[t._v('[^2] Srivastava et al, "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", 2014')]),t._v(" "),a("hr",{staticClass:"footnotes-sep"}),t._v(" "),a("section",{staticClass:"footnotes"},[a("ol",{staticClass:"footnotes-list"},[a("li",{staticClass:"footnote-item",attrs:{id:"fn1"}},[a("p",[t._v('Sergey Ioffe and Christian Szegedy, "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", ICML 2015. '),a("a",{staticClass:"footnote-backref",attrs:{href:"#fnref1"}},[t._v("↩︎")]),t._v(" "),a("a",{staticClass:"footnote-backref",attrs:{href:"#fnref1:1"}},[t._v("↩︎")])])])])])])}),[],!1,null,null,null);e.default=r.exports}}]);