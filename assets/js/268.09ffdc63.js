(window.webpackJsonp=window.webpackJsonp||[]).push([[268],{702:function(t,e,a){"use strict";a.r(e);var i=a(2),s=Object(i.a)({},(function(){var t=this,e=t._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h2",{attrs:{id:"unceratinty"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#unceratinty"}},[t._v("#")]),t._v(" Unceratinty")]),t._v(" "),e("p",[t._v("Uncertainty: the lack of confidence for each output")]),t._v(" "),e("ul",[e("li",[t._v("Epistemic uncertainty: uncertainty due to a lack of knowledge, stemming from observing only a subset of all possible data points")]),t._v(" "),e("li",[t._v("Aleatoric uncertainty: captures inherent stochasticity in the dataset, for example due to points that lie on the decision boundary between two classes")])]),t._v(" "),e("img",{staticClass:"medium-zoom",attrs:{src:"/idp/aleatoric_epistemic_uncertainty.png",alt:"aleatoric and epistemic and uncertainty",width:"100%"}}),t._v(" "),e("p",[t._v("According to "),e("sup",{staticClass:"footnote-ref"},[e("a",{attrs:{href:"#fn1",id:"fnref1"}},[t._v("[1]")])]),t._v(", Uncertainties are epistemic, if the modeler sees a possibility to reduce them by gathering more data or by refining models. Uncertainties are categorized as aleatory if the modeler does not foresee the possibility of reducing them.")]),t._v(" "),e("h2",{attrs:{id:"interpretability-vs-explainability"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#interpretability-vs-explainability"}},[t._v("#")]),t._v(" Interpretability vs. Explainability")]),t._v(" "),e("p",[t._v("In "),e("sup",{staticClass:"footnote-ref"},[e("a",{attrs:{href:"#fn2",id:"fnref2"}},[t._v("[2]")])]),t._v(", interpretability, explainability, and the state-of-the-art examples are reviewed.")]),t._v(" "),e("p",[t._v('Interpretability of ML systems is "the ability to explain or to present in understandable terms to a human" '),e("sup",{staticClass:"footnote-ref"},[e("a",{attrs:{href:"#fn3",id:"fnref3"}},[t._v("[3]")])]),t._v(".")]),t._v(" "),e("p",[t._v("Interpretable ML focuses on designing models that are inherently interpretable. Explainable ML tries to provide "),e("strong",[t._v("post hoc")]),t._v(" explanations for existing "),e("strong",[t._v("black box")]),t._v(" models "),e("sup",{staticClass:"footnote-ref"},[e("a",{attrs:{href:"#fn4",id:"fnref4"}},[t._v("[4]")])]),t._v(".")]),t._v(" "),e("p",[t._v('Interpretability raises the question "How does the model work?"; whereas explanation methods try to answer "What else can the model tell me?"')]),t._v(" "),e("img",{staticClass:"medium-zoom",attrs:{src:"/idp/taxonomy_interpretable_explainable.png",alt:"A taxonomy of interpretable and explainable ML techniques",width:"100%"}}),t._v(" "),e("h2",{attrs:{id:"explanation"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#explanation"}},[t._v("#")]),t._v(" Explanation")]),t._v(" "),e("p",[t._v("Feature importance estimation:")]),t._v(" "),e("ol",[e("li",[t._v("gradient-based methods: only applicable to differentiable models, such as NN; fast computation")]),t._v(" "),e("li",[t._v("methods based on sensitivity analysis: quantify a model’s sensitivity to changes in the input, such as LIME, SHAP, Kernel SHAP; slow to compute, as large numbers of model evaluations are necessary to assess a model’s sensitivity")]),t._v(" "),e("li",[t._v("methods that measure the change in model confidence when removing input features: mask parts of the input and measure the model’s resulting change in conﬁdence")]),t._v(" "),e("li",[t._v("mimic models: train interpretable models that mimic black-box model")])]),t._v(" "),e("p",[e("strong",[t._v("Cxplain")])]),t._v(" "),e("h2",{attrs:{id:"counterfactual-explanations"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#counterfactual-explanations"}},[t._v("#")]),t._v(" Counterfactual Explanations")]),t._v(" "),e("p",[t._v("Try to answer: Why this prediction was made instead of another one?")]),t._v(" "),e("p",[t._v("Such techniques focus on producing "),e("em",[t._v("contrastive")]),t._v(" and "),e("em",[t._v("actionable local")]),t._v(" explanations.")]),t._v(" "),e("p",[t._v("Interpretability community: how input variables must be modiﬁed to change a model’s output without making explicit causal assumptions "),e("sup",{staticClass:"footnote-ref"},[e("a",{attrs:{href:"#fn5",id:"fnref5"}},[t._v("[5]")])]),t._v(".")]),t._v(" "),e("p",[t._v("According to "),e("sup",{staticClass:"footnote-ref"},[e("a",{attrs:{href:"#fn6",id:"fnref6"}},[t._v("[6]")])]),t._v(", CEs should have the following properties:")]),t._v(" "),e("ul",[e("li",[t._v("Minimal Perturbation")]),t._v(" "),e("li",[t._v("Realistic Explanation")]),t._v(" "),e("li",[t._v("Unambiguous Explanation")]),t._v(" "),e("li",[t._v("Realistic or Actionable Perturbation")]),t._v(" "),e("li",[t._v("Run Time of the Algorithm (suﬃciently quickly)")])]),t._v(" "),e("p",[e("strong",[t._v("CLUE")])]),t._v(" "),e("h2",{attrs:{id:"references"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#references"}},[t._v("#")]),t._v(" References")]),t._v(" "),e("hr",{staticClass:"footnotes-sep"}),t._v(" "),e("section",{staticClass:"footnotes"},[e("ol",{staticClass:"footnotes-list"},[e("li",{staticClass:"footnote-item",attrs:{id:"fn1"}},[e("p",[t._v("Aleatory or epistemic? Does it matter? "),e("a",{staticClass:"footnote-backref",attrs:{href:"#fnref1"}},[t._v("↩︎")])])]),t._v(" "),e("li",{staticClass:"footnote-item",attrs:{id:"fn2"}},[e("p",[t._v("Interpretability and Explainability: A Machine Learning Zoo Mini-tour "),e("a",{staticClass:"footnote-backref",attrs:{href:"#fnref2"}},[t._v("↩︎")])])]),t._v(" "),e("li",{staticClass:"footnote-item",attrs:{id:"fn3"}},[e("p",[t._v("F. Doshi-Velez and B. Kim, “Towards a rigorous science of interpretable machine learning,” 2017. arXiv:1702.08608. "),e("a",{staticClass:"footnote-backref",attrs:{href:"#fnref3"}},[t._v("↩︎")])])]),t._v(" "),e("li",{staticClass:"footnote-item",attrs:{id:"fn4"}},[e("p",[t._v("C. Rudin, “Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead,” Nature Machine Intelligence, vol. 1, no. 5, pp. 206–215, 2019. "),e("a",{staticClass:"footnote-backref",attrs:{href:"#fnref4"}},[t._v("↩︎")])])]),t._v(" "),e("li",{staticClass:"footnote-item",attrs:{id:"fn5"}},[e("p",[t._v("Sandra Wachter, Brent Mittelstadt, and Chris Russell. Counterfactual explanations without opening the black box: Automated decisions and the GDPR. Harvard Journal of Law & Technology, 31(2), 2018. "),e("a",{staticClass:"footnote-backref",attrs:{href:"#fnref5"}},[t._v("↩︎")])])]),t._v(" "),e("li",{staticClass:"footnote-item",attrs:{id:"fn6"}},[e("p",[t._v("Generating Interpretable Counterfactual Explanations By Implicit Minimisation of Epistemic and Aleatoric Uncertainties "),e("a",{staticClass:"footnote-backref",attrs:{href:"#fnref6"}},[t._v("↩︎")])])])])])])}),[],!1,null,null,null);e.default=s.exports}}]);